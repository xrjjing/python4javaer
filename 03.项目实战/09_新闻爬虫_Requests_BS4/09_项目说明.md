# 09_新闻爬虫（Requests + BeautifulSoup）

## 目标
- 演示最小可用的新闻标题爬取流程。
- 支持离线运行：默认解析本地 `sample_news.html`，避免真实网络依赖。
- 解析与网络层解耦，便于单元测试与 Mock。

## 环境依赖
- `requests`、`beautifulsoup4` 已在仓库 requirements 中。
- 无需部署数据库/缓存；可在离线环境运行。

## 关键文件
- `crawler_news_bs4.py`：爬虫主脚本，`parse_articles` 负责解析；`fetch_html` 可联网抓取。
- `sample_news.html`：本地示例页面，默认输入。
- `test_crawler_news_bs4.py`：单测，使用本地 HTML，避免外网。
- `crawler_news_httpx.py`：异步版本，内置简单 RateLimiter 与 robots 检查，演示 httpx + asyncio。
- `高级教程.md`：爬虫最佳实践（合规、限速、代理、解析策略）。

### 基础版 vs 进阶版
| 版本 | 位置 | 特性 |
|---|---|---|
| 基础版 | `03.项目实战/05_简单爬虫_新闻标题抓取` | 同步 requests + BS4，最少代码，依赖真实 URL |
| 进阶版 | `03.项目实战/09_新闻爬虫_Requests_BS4` | 离线 sample、异步 httpx、限速、robots 检查、单测完备 |

## 快速运行
```bash
# 默认离线（同步版）：解析 sample_news.html
python 03.项目实战/09_新闻爬虫_Requests_BS4/crawler_news_bs4.py

# 指定本地 HTML
python crawler_news_bs4.py /path/to/page.html

# 指定 URL（同步版，请遵守 robots）
python crawler_news_bs4.py https://example.com/news

# 异步版（httpx + RateLimiter + robots 检查）
python crawler_news_httpx.py https://example.com/news
```

## 开发要点
- 使用 `User-Agent`，生产环境需遵守 robots.txt 与网站协议。
- 解析层只依赖传入的 HTML 字符串，便于 Mock。
- 结果写入 `titles.txt`，可替换为数据库/消息队列等。
- 需要多页/高并发抓取时，优先参考 `crawler_news_httpx.py` 的限速与 robots 检查。

## 测试
```bash
pytest 03.项目实战/09_新闻爬虫_Requests_BS4/test_crawler_news_bs4.py -q
```

## 扩展思路
- 增加分页抓取与去重策略。
- 加入速率限制、重试与代理池。
- 输出 JSON/CSV 或推送到对象存储（可结合 tests/fixtures/fake_minio）。
