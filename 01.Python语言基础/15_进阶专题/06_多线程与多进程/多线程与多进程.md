# 多线程与多进程

> 本专题的目标不是让你一口气精通并发编程，而是站在「实用 + 对比」的角度，帮你搞清楚：什么时候该用多线程、什么时候用多进程，以及它们和前面学过的 `asyncio` 有什么区别。

配套示例代码：

- `01.Python语言基础/15_进阶专题/06_多线程与多进程/demo_concurrency.py`
- 建议后续补充：`practice_concurrency.py` 作为练习题文件

---

## 1. 概念扫盲：并发 vs 并行、CPU 密集 vs IO 密集

在动手写代码之前，先用最简洁的方式把几个容易混的概念区分开：

- **并发（Concurrency）**：看起来「同时」在做多件事，实际上可能是一个 CPU 在不同任务之间来回切换（时间片）。  
- **并行（Parallelism）**：真正意义上的「同时」，例如多核 CPU 上多个进程/线程各占一个核一起跑。
- **CPU 密集型任务**：主要消耗 CPU 计算时间，比如大规模数字运算、加密、图像处理、复杂算法等。
- **IO 密集型任务**：主要时间花在等待 IO 上，比如网络请求、磁盘读写、数据库访问等。

在 CPython 中，由于 GIL（全局解释器锁）的存在：

- 多线程更适合 **IO 密集型任务**（比如并发请求多个 HTTP 接口）；
- 多进程更适合 **CPU 密集型任务**（比如并行计算大量数据）。

`asyncio` 则适合大量 IO 密集且对高并发有要求的场景（例如成百上千个并发网络请求），你在「异步编程入门」专题中已经见过。

---

## 2. 用 ThreadPoolExecutor 并发请求多个 HTTP 接口（IO 密集）

最常见、最实用的多线程场景：**同时请求多个外部 HTTP 接口**。  
例如：调用多个微服务、批量查询第三方平台接口等。

在 `demo_concurrency.py` 中，你会看到类似这样的示例：

```python
from concurrent.futures import ThreadPoolExecutor

def fetch_status(url: str, timeout: float = 5.0) -> tuple[str, int | None]:
    \"\"\"请求 URL 并返回 (url, 状态码)。\"\"\"
    ...

def fetch_all_status(urls: list[str], max_workers: int = 5) -> list[tuple[str, int | None]]:
    \"\"\"使用线程池并发请求一组 URL。\"\"\"
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(fetch_status, url) for url in urls]
        return [f.result() for f in futures]
```

建议你对比以下三种写法的性能和代码可读性：

1. 单线程 for 循环逐个请求；
2. `ThreadPoolExecutor` 实现粗粒度并发；
3. 使用 `asyncio`（参考「异步编程入门」示例）实现更高并发。

一般经验：

- 少量并发（几十个请求以内）：`ThreadPoolExecutor` 足够简单好用；
- 大量并发（上百甚至上千）：考虑 `asyncio` + `aiohttp` 之类的方案。

---

## 3. 用 ProcessPoolExecutor 处理 CPU 密集任务（计算密集）

如果你要做的是「大量计算」，例如：

- 为一大批数据计算复杂指标；
- 在脚本中做一些离线分析（如统计、加密等）；

此时多线程由于 GIL 的限制帮助有限，可以考虑 `multiprocessing` 或 `ProcessPoolExecutor`。

示例（见 `demo_concurrency.py`）：

```python
from concurrent.futures import ProcessPoolExecutor

def count_primes(n: int) -> int:
    \"\"\"粗略计算 [2, n) 范围内的素数个数（示例用，不追求高效算法）。\"\"\"
    ...

def count_primes_in_ranges(ranges: list[tuple[int, int]], max_workers: int = 4) -> list[int]:
    \"\"\"使用进程池并行计算多个区间内的素数个数。\"\"\"
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(count_primes, end) for _, end in ranges]
        return [f.result() for f in futures]
```

注意事项：

- **Windows 下必须放在 `if __name__ == "__main__":` 保护下**，否则可能出现无限递归创建子进程的问题；  
- 子进程之间不共享内存，适合「输入参数比较小、计算过程独立、结果相对简短」的场景。

---

## 4. 与 asyncio 的简单对比：该用谁？

在本仓库中你已经学过：

- `15_进阶专题/07_异步编程入门/demo_async_programming.py`

可以简单记住下面这张「脑图」：

- IO 密集型：
  - 少量并发：`ThreadPoolExecutor`（简单）；  
  - 大量并发：`asyncio`（需要更多心智负担，但更高效）。
- CPU 密集型：
  - 使用 `ProcessPoolExecutor` 或 `multiprocessing`。

在 Web 项目中（例如 FastAPI）：

- 异步端点函数内部可以使用 `asyncio` 做 IO 并发；  
- CPU 密集型任务尽量放到后台任务队列或单独的进程/服务中处理。

你可以尝试在阅读 `03.项目实战/07_监控数据处理_指标聚合与告警` 时思考：  
如果指标计算变得很重，是否有必要用多进程加速，或者拆成离线任务。

---

## 5. 在脚本/服务中的常见使用模式

综合来看，线程/进程在脚本和服务中的使用模式可以概括为：

1. **简易并发脚本**  
   - 用 ThreadPool 批量调用 API、下载文件、处理日志文件等；
2. **离线计算加速**  
   - 用 ProcessPool 在多核上跑 CPU 密集分析任务；
3. **与现有代码集成**  
   - 将「耗时任务」封装成函数，再用线程/进程池调度；
   - 保持主线程逻辑简单，易于测试和维护。

在 `demo_concurrency.py` 中，代码会尽量采用「小函数 + 池」的形式，方便你在自己的脚本里直接借鉴这种结构。

---

## 6. 建议练习（可以在 practice_concurrency.py 中实现）

如果你想进一步巩固本专题的内容，可以尝试完成如下小练习：

1. **并发下载多个网页并统计标题长度**  
   - 给定多个 URL，使用 `ThreadPoolExecutor` 并发下载；  
   - 解析 `<title>` 文本并统计长度，返回列表。
2. **用进程池加速简单统计任务**  
   - 编写一个函数，统计给定整数 n 以内所有数字的位数和（例如 123 → 1+2+3）；  
   - 使用 `ProcessPoolExecutor` 对多个 n 并行计算总和，并验证与单线程结果一致。
3. **对比 asyncio 与线程池的行为**  
   - 任选一个 IO 密集任务（例如请求 `https://httpbin.org/delay/1` 多次）；  
   - 分别用 `ThreadPoolExecutor` 和 `asyncio` 实现，并比较代码复杂度与运行时间。

完成这些练习后，再回头看「异步编程入门」和 `03.项目实战` 中的项目，你会对不同并发模型的优缺点和适用场景有更加直观的理解。

---

## 在本仓库中的应用

- **integration_gateway_service**：网关服务使用 `asyncio` 而非线程池处理并发请求，适合高 IO 场景
- **批量数据处理脚本**：如需批量调用 API 或处理大量文件，可使用 `ThreadPoolExecutor`
- **CPU 密集型离线任务**：如日志统计、数据聚合等，可使用 `ProcessPoolExecutor` 加速
- **FastAPI 异步路由**：各服务的 `async def` 端点利用事件循环处理并发，与线程池形成对比

**适用场景对比**：

| 场景 | 推荐方案 | 本仓库示例 |
|------|----------|-----------|
| 并发 HTTP 请求（少量） | `ThreadPoolExecutor` | 批量调用后端 API |
| 并发 HTTP 请求（大量） | `asyncio` + `httpx` | integration_gateway_service |
| CPU 密集计算 | `ProcessPoolExecutor` | 日志聚合统计 |
| Web 服务端点 | `async def` | FastAPI 路由 |

**Java 对比**：
- Python `ThreadPoolExecutor` ≈ Java `ExecutorService.newFixedThreadPool()`
- Python `ProcessPoolExecutor` ≈ Java `ForkJoinPool`
- Python GIL 限制 ≈ Java 无此限制（JVM 原生多线程）

## 🎯 开放式挑战

### 挑战 1：智能网页爬虫性能对比（初级）

**任务描述**：
实现一个多线程网页爬虫，爬取指定网站列表的标题和元描述，并对比不同并发模型的性能差异。

**要求**：
1. 使用以下三种方式分别实现：
   - 单线程顺序爬取
   - `ThreadPoolExecutor`（线程池大小：5）
   - `asyncio` + `aiohttp`

2. 爬取以下信息：
   - 网页标题（`<title>` 标签）
   - Meta 描述（`<meta name="description">`）
   - 响应时间（毫秒）

**测试数据**：
```python
urls = [
    "https://www.python.org",
    "https://docs.python.org",
    "https://pypi.org",
    "https://github.com",
    "https://stackoverflow.com",
    # ... 可扩展至 20-30 个 URL
]
```

**预期输出**：
```
=== 单线程顺序 ===
总耗时: 15.2秒
平均响应: 760ms

=== ThreadPoolExecutor ===
总耗时: 3.8秒
平均响应: 190ms
加速比: 4.0x

=== asyncio + aiohttp ===
总耗时: 2.1秒
平均响应: 105ms
加速比: 7.2x
```

**提示**：
- 参考本文档 [2. ThreadPoolExecutor](#2-用-threadpoolexecutor-并发请求多个-http-接口io-密集)
- 使用 `BeautifulSoup` 或 `lxml` 解析 HTML
- 记录每个请求的开始/结束时间
- 起点代码：`03.项目实战/05_简单爬虫_新闻标题抓取/`

**扩展要求**：
- 添加重试机制（失败自动重试 3 次）
- 实现请求频率限制（避免被封 IP）
- 生成性能对比图表（使用 matplotlib）

---

### 挑战 2：分布式日志分析器（中级）

**任务描述**：
构建一个多进程日志分析工具，对大型日志文件进行并行处理和统计。

**场景**：
有一个 1GB 的 Web 服务器访问日志，需要统计：
- 每小时的请求数量（24 小时分布图）
- 不同 HTTP 状态码的占比（200、404、500 等）
- 访问量 Top 10 的 IP 地址
- 平均响应时间

**技术要求**：
1. 使用 `ProcessPoolExecutor` 将日志文件分块处理
2. 每个进程处理一块数据并返回统计结果
3. 主进程汇总所有子进程的结果
4. 处理时间需在 10 秒内完成（相比单线程加速 4x+）

**日志格式示例**：
```
192.168.1.100 - - [15/Jan/2025:10:30:45 +0800] "GET /api/users HTTP/1.1" 200 1234 0.052
192.168.1.101 - - [15/Jan/2025:10:31:20 +0800] "POST /api/login HTTP/1.1" 401 89 0.015
```

**实现要点**：
```python
from concurrent.futures import ProcessPoolExecutor
from collections import Counter
import re

def analyze_log_chunk(lines: list[str]) -> dict:
    """分析日志块，返回统计结果"""
    stats = {
        'hourly_requests': Counter(),
        'status_codes': Counter(),
        'ip_counts': Counter(),
        'response_times': []
    }
    # ... 处理逻辑
    return stats

def merge_stats(all_stats: list[dict]) -> dict:
    """合并所有子进程的统计结果"""
    # ... 合并逻辑
    pass

# 主流程
def analyze_log_file(filepath: str, num_workers: int = 4):
    # 读取并分块
    chunks = split_file_into_chunks(filepath, num_workers)

    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        results = executor.map(analyze_log_chunk, chunks)

    return merge_stats(list(results))
```

**预期难点**：
- 大文件如何高效分块（不能破坏日志行完整性）
- 子进程间不共享内存，如何设计数据结构
- 如何处理进程间的数据序列化开销

**提示**：
- 参考 [3. ProcessPoolExecutor](#3-用-processpoolexecutor-处理-cpu-密集任务计算密集)
- 起点代码：`03.项目实战/02_日志分析与报表生成/`
- 使用 `mmap` 或按字节偏移分块读取文件

**评估标准**：
- 正确性：统计结果与单线程一致
- 性能：相比单线程加速 3x 以上
- 可扩展性：支持任意大小的日志文件

---

### 挑战 3：智能任务调度系统（高级）

**任务描述**：
实现一个混合并发的任务调度系统，自动识别任务类型并选择最优执行策略。

**系统功能**：
1. **任务分类**：
   - CPU 密集型：图像处理、数据加密、复杂计算
   - IO 密集型：HTTP 请求、文件读写、数据库查询
   - 混合型：既有计算又有 IO

2. **智能调度**：
   - CPU 密集 → `ProcessPoolExecutor`
   - IO 密集（少量）→ `ThreadPoolExecutor`
   - IO 密集（大量）→ `asyncio`
   - 混合型 → 动态调整

3. **资源管理**：
   - 根据系统负载自动调整 worker 数量
   - 任务队列优先级管理
   - 实时监控任务执行状态

**核心接口设计**：
```python
from enum import Enum
from typing import Callable, Any

class TaskType(Enum):
    CPU_INTENSIVE = "cpu"
    IO_INTENSIVE = "io"
    MIXED = "mixed"

class Task:
    def __init__(self, func: Callable, args: tuple, task_type: TaskType):
        self.func = func
        self.args = args
        self.task_type = task_type
        self.priority = 0

class TaskScheduler:
    def __init__(self, max_cpu_workers: int = 4, max_io_workers: int = 10):
        self.cpu_pool = ProcessPoolExecutor(max_workers=max_cpu_workers)
        self.io_pool = ThreadPoolExecutor(max_workers=max_io_workers)
        self.task_queue = []

    def submit(self, task: Task) -> Future:
        """提交任务并返回 Future 对象"""
        if task.task_type == TaskType.CPU_INTENSIVE:
            return self.cpu_pool.submit(task.func, *task.args)
        elif task.task_type == TaskType.IO_INTENSIVE:
            return self.io_pool.submit(task.func, *task.args)
        else:
            # 混合型任务的智能调度逻辑
            return self._schedule_mixed_task(task)

    def _schedule_mixed_task(self, task: Task):
        """动态分析混合型任务的特征并选择执行器"""
        # 实现启发式算法
        pass

    def get_stats(self) -> dict:
        """返回调度器统计信息"""
        return {
            'cpu_active': self.cpu_pool._threads,
            'io_active': self.io_pool._threads,
            'queue_length': len(self.task_queue)
        }
```

**高级特性**：
1. **自适应调整**：
   - 监控 CPU 使用率，动态调整进程池大小
   - 根据任务响应时间调整线程池大小

2. **任务依赖**：
   - 支持任务间的依赖关系（Task A 必须在 Task B 完成后执行）
   - 使用 DAG（有向无环图）管理依赖

3. **故障恢复**：
   - 任务执行失败自动重试
   - 记录失败原因和重试次数
   - 超时任务自动终止

**测试场景**：
```python
# 场景 1：批量图像处理 + 结果上传
tasks = [
    Task(resize_image, (img_path,), TaskType.CPU_INTENSIVE),
    Task(upload_to_s3, (processed_img,), TaskType.IO_INTENSIVE),
]

# 场景 2：爬虫 + 数据分析
tasks = [
    Task(fetch_html, (url,), TaskType.IO_INTENSIVE),
    Task(extract_data, (html,), TaskType.CPU_INTENSIVE),
]

scheduler = TaskScheduler()
futures = [scheduler.submit(task) for task in tasks]
results = [f.result() for f in futures]
```

**性能目标**：
- 相比单纯使用一种执行器，整体吞吐量提升 30%+
- 支持 1000+ 并发任务不崩溃
- 内存占用保持在合理范围（< 500MB）

**提示**：
- 参考 Celery、RQ 等任务队列的设计思路
- 使用 `psutil` 监控系统资源
- 可以参考 `03.项目实战/B_批处理任务_定时与脚本/` 的任务管理逻辑

---

### 挑战 4：实时数据流处理引擎（综合实战）

**任务描述**：
构建一个生产级的实时数据流处理系统，模拟真实业务场景。

**业务场景**：
电商平台实时订单处理系统，需要并发处理：
1. 订单创建（IO：写数据库）
2. 库存扣减（CPU：并发控制 + 锁）
3. 支付验证（IO：调用第三方 API）
4. 物流推送（IO：异步通知）
5. 数据统计（CPU：实时聚合）

**系统架构**：
```
订单流入 → 预处理（线程池）→ 分流处理
                           ↓
                    ┌──────┴──────┐
                    ↓             ↓
              CPU 密集       IO 密集
            （进程池）      （asyncio）
                    ↓             ↓
                    └──────┬──────┘
                           ↓
                      结果汇总与持久化
```

**技术栈要求**：
- `asyncio` 处理高并发 IO（支付、物流）
- `ProcessPoolExecutor` 处理库存锁竞争
- `ThreadPoolExecutor` 处理数据库写入
- `multiprocessing.Queue` 实现进程间通信
- Redis 作为分布式锁

**核心功能**：
1. **订单流水线**：
   ```python
   async def process_order_pipeline(order_id: str):
       # 阶段1：验证订单（IO）
       order = await validate_order(order_id)

       # 阶段2：库存扣减（CPU 密集 + 锁）
       stock_ok = await reduce_stock_with_lock(order.items)

       # 阶段3：支付验证（IO）
       payment_ok = await verify_payment(order.payment_id)

       # 阶段4：物流推送（IO）
       await push_logistics(order_id)

       # 阶段5：统计更新（CPU）
       await update_realtime_stats(order)
   ```

2. **实时监控大屏**：
   - 每秒订单处理量（TPS）
   - 各阶段平均耗时
   - 失败率与重试率
   - 活跃 worker 数量

3. **压力测试**：
   - 模拟每秒 1000 笔订单涌入
   - 系统需保证 99% 订单在 2 秒内完成
   - 无订单丢失或重复处理

**评估指标**：
- **正确性**：无订单丢失、库存准确、支付幂等
- **性能**：TPS > 1000，P99 延迟 < 2s
- **稳定性**：连续运行 1 小时无崩溃
- **可观测性**：完善的日志与指标

**提示**：
- 参考 `03.项目实战/integration_gateway_service/` 的异步架构
- 使用 `aioredis` 实现分布式锁
- 借鉴 `rbac_auth_service` 的数据库事务处理
- 可以使用 Locust 进行压力测试

---

## 相关资源

- [Python官方文档 - concurrent.futures](https://docs.python.org/zh-cn/3/library/concurrent.futures.html)
- [Python官方文档 - multiprocessing](https://docs.python.org/zh-cn/3/library/multiprocessing.html)
- [Real Python - Python Concurrency](https://realpython.com/python-concurrency/)
- [Java vs Python 并发模型对比](../../../docs/Java_vs_Python_CheatSheet.md#2-并发模型对比)
